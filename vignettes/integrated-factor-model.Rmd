---
title: "R Notebook"
output: html_notebook
---

- can't run this in bombas or any recast b/c recast project is pinned to rstan 2.18
- this model requires stanheaders 2.19.2
- stanheaders can't be above rstan

```{r}
library(tidyverse)
library(rstan)
library(bayesplot)
weekly_clean_data = read_csv("../../weekly_clean_data.csv")
dfa_pred = stan_model("../src/stan_files/dfa-pred.stan")
stanmodels <- list()
stanmodels$dfa_pred = dfa_pred
```


- have to include the predictors matrix into the stan object (rn it is y)
- have to set z-score = F (what will that do)
- 
- 

```{r}
#' Fit a Bayesian DFA
#'
#' @param y A matrix of data to fit. See `data_shape` option to specify whether
#'   this is long or wide format data.
#' @param num_trends Number of trends to fit.
#' @param varIndx Indices indicating which timeseries should have shared
#'   variances.
#' @param zscore Logical. Should the data be standardized first? If not it is
#'   just centered. Centering is necessary because no intercept is included.
#' @param iter Number of iterations in Stan sampling, defaults to 2000.
#' @param thin Thinning rate in Stan sampling, defaults to 1.
#' @param chains Number of chains in Stan sampling, defaults to 4.
#' @param control A list of options to pass to Stan sampling. Defaults to
#'   `list(adapt_delta = 0.99, max_treedepth = 20)`.
#' @param nu_fixed Student t degrees of freedom parameter. If specified as
#'   greater than 100, a normal random walk is used instead of a random walk
#'   with a t-distribution. Defaults to `101`.
#' @param est_correlation Boolean, whether to estimate correlation of
#'   observation error matrix `R`. Defaults to `FALSE`.
#' @param estimate_nu Logical. Estimate the student t degrees of freedom
#'   parameter? Defaults to `FALSE`,
#' @param estimate_trend_ar Logical. Estimate AR(1) parameters on DFA trends?
#'   Defaults to `FALSE``, in which case AR(1) parameters are set to 1
#' @param estimate_trend_ma Logical. Estimate MA(1) parameters on DFA trends?
#'   Defaults to `FALSE``, in which case MA(1) parameters are set to 0.
#' @param sample Logical. Should the model be sampled from? If `FALSE`, then the
#'   data list object that would have been passed to Stan is returned instead.
#'   This is useful for debugging and simulation. Defaults to `TRUE`.
#' @param data_shape If `wide` (the current default) then the input data should
#'   have rows representing the various timeseries and columns representing the
#'   values through time. This matches the MARSS input data format. If `long`
#'   then the input data should have columns representing the various timeseries
#'   and rows representing the values through time.
#' @param obs_covar Optional dataframe of data with 4 named columns ("time","timeseries","covariate","value"), representing: (1) time, (2) the time series
#'   affected, (3) the covariate number for models with more than one covariate affecting each
#'   trend, and (4) the value of the covariate
#' @param pro_covar Optional dataframe of data with 4 named columns ("time","trend","covariate","value"), representing: (1) time, (2) the trend
#'   affected, (3) the covariate number for models with more than one covariate affecting each
#'   trend, and (4) the value of the covariate
#' @param ... Any other arguments to pass to [rstan::sampling()].
#' @details Note that there is nothing restricting the loadings and trends from
#'   being inverted (i.e. multiplied by `-1`) for a given chain. Therefore, if
#'   you fit multiple chains, the package will attempt to determine which chains
#'   need to be inverted using the function [find_inverted_chains()].
#' @seealso plot_loadings plot_trends rotate_trends find_swans
#'
#' @export
#'
#' @importFrom rstan sampling
#' @import Rcpp
#' @importFrom graphics lines par plot points polygon segments
#' @importFrom stats na.omit
#'
#' @examples
#' set.seed(42)
#' s <- sim_dfa(num_trends = 1, num_years = 20, num_ts = 3)
#' # only 1 chain and 250 iterations used so example runs quickly:
#' m <- fit_dfa(y = s$y_sim, iter = 250, chains = 1)
#'\dontrun{
#' # example of observation error covariates
#' obs_covar = expand.grid("time"=1:20,"timeseries"=1:3,"covariate"=1)
#' obs_covar$value=rnorm(nrow(obs_covar),0,0.1)
#' m <- fit_dfa(y = s$y_sim, iter = 250, chains = 1, obs_covar=obs_covar)
#'
#' # example of process error covariates
#' pro_covar = expand.grid("time"=1:20,"trend"=1:3,"covariate"=1)
#' pro_covar$value=rnorm(nrow(pro_covar),0,0.1)
#' m <- fit_dfa(y = s$y_sim, iter = 250, chains = 1, pro_covar=pro_covar)
#'}

fit_dfa_pred <- function(y,
                         depvar,
                    num_trends = 1,
                    varIndx = NULL,
                    zscore = T,
                    iter = 2000,
                    chains = 4,
                    thin = 1,
                    control = list(adapt_delta = 0.99, max_treedepth = 20),
                    nu_fixed = 101,
                    est_correlation = FALSE,
                    estimate_nu = FALSE,
                    estimate_trend_ar = FALSE,
                    estimate_trend_ma = FALSE,
                    sample = TRUE,
                    data_shape = c("wide", "long"),
                    obs_covar = NULL,
                    pro_covar = NULL,
                    ...) {
  predictors <- y
  data_shape <- match.arg(data_shape)
  if (ncol(y) > nrow(y) && data_shape == "long") {
    warning(
      "ncol(y) > nrow(y) and data_shape == 'long'; are you sure your",
      "input data is in long format?"
    )
  }
  if (ncol(y) < nrow(y) && data_shape == "wide") {
    warning(
      "ncol(y) < nrow(y) and data_shape == 'wide'; are you sure your",
      "input data is in wide format?"
    )
  }
  if (data_shape == "long") {
    y <- t(y)
  }

  if (nrow(y) < 3) {
    stop("fit_dfa() only works with 3 or more time series. We detected ",
      nrow(y), " time series.")
  }

  if(!is.null(obs_covar)) {
    if(ncol(obs_covar) != 4) {
      stop("observation covariates must be in a data frame with 4 columns")
    }
  }
  if(!is.null(pro_covar)) {
    if(ncol(pro_covar) != 4) {
      stop("process covariates must be in a data frame with 4 columns")
    }
  }

  # parameters for DFA
  N <- ncol(y) # number of time steps
  P <- nrow(y) # number of time series
  K <- num_trends # number of dfa trends
  nZ <- P * K - sum(seq_len(K)) # number of non-zero parameters that are unconstrained

  for (i in seq_len(P)) {
    if (zscore) {
      if (length(unique(na.omit(c(y[i, ])))) == 1L) {
        stop("Can't scale one or more of the time series because all values ",
          "are the same. Remove this/these time series or set `zscore = FALSE`.",
          call. = FALSE
        )
      }
      y[i, ] <- scale(y[i, ], center = TRUE, scale = TRUE)
    } else {
      y[i, ] <- scale(y[i, ], center = TRUE, scale = FALSE)
    }
  }
  Y <- y # included in returned object at end

  # mat_indx now references the unconstrained values of the Z matrix.
  mat_indx <- matrix(0, P, K)
  start <- 1
  for (k in seq_len(K)) {
    for (p in seq(k + 1, P)) {
      mat_indx[p, k] <- start
      start <- start + 1
    }
  }
  # row_indx and col_indx now references the unconstrained values of the Z matrix.
  row_indx <- matrix((rep(seq_len(P), K)), P, K)[mat_indx > 0]
  col_indx <- matrix(sort(rep(seq_len(K), P)), P, K)[mat_indx > 0]

  diag(mat_indx) <- 1
  row_indx_z <- matrix((rep(seq_len(P), K)), P, K)[mat_indx == 0]
  col_indx_z <- matrix(sort(rep(seq_len(K), P)), P, K)[mat_indx == 0]
  row_indx_z <- c(row_indx_z, 0, 0) # +2 zeros for making stan ok with data types
  col_indx_z <- c(col_indx_z, 0, 0) # +2 zeros for making stan ok with data types
  nZero <- length(row_indx_z)

  # set the model up to have shared variances
  if (is.null(varIndx)) {
    varIndx <- rep(1, P)
  }
  nVariances <- length(unique(varIndx))

  # indices of positive values - Stan can't handle NAs
  row_indx_pos <- matrix(rep(seq_len(P), N), P, N)[!is.na(y)]
  col_indx_pos <- matrix(sort(rep(seq_len(N), P)), P, N)[!is.na(y)]
  n_pos <- length(row_indx_pos)

  row_indx_na <- matrix(rep(seq_len(P), N), P, N)[is.na(y)]
  col_indx_na <- matrix(sort(rep(seq_len(N), P)), P, N)[is.na(y)]
  n_na <- length(row_indx_na)

  y <- y[!is.na(y)]

  # flag for whether to use a normal dist
  use_normal <- if (nu_fixed > 100) 1 else 0
  if (estimate_nu) use_normal <- 0 # competing flags

  # covariates
  if(!is.null(obs_covar)) {
    obs_covar_index = as.matrix(obs_covar[,c("time","timeseries","covariate")])
    num_obs_covar = nrow(obs_covar_index)
    n_obs_covar = length(unique(obs_covar_index[,"covariate"]))
    obs_covar_value = obs_covar[,"value"]
  } else {
    num_obs_covar = 0
    n_obs_covar = 0
    obs_covar_value = c(0)[0]
    obs_covar_index = matrix(0,1,3)[c(0)[0],]
  }
  if(!is.null(pro_covar)) {
    pro_covar_index = as.matrix(pro_covar[,c("time","trend","covariate")])
    num_pro_covar = nrow(pro_covar_index)
    n_pro_covar = length(unique(pro_covar_index[,"covariate"]))
    pro_covar_value = pro_covar[,"value"]
  } else {
    num_pro_covar = 0
    n_pro_covar = 0
    pro_covar_value = c(0)[0]
    pro_covar_index = matrix(0,1,3)[c(0)[0],]
  }

  data_list <- list(
    N = N,
    P = P,
    K = K,
    nZ = nZ,
    y = y,
    row_indx = row_indx,
    col_indx = col_indx,
    nZero = nZero,
    varIndx = varIndx,
    nVariances = nVariances,
    row_indx_z = row_indx_z,
    col_indx_z = col_indx_z,
    nZero = nZero,
    row_indx_z = row_indx_z,
    col_indx_z = col_indx_z,
    row_indx_pos = row_indx_pos,
    col_indx_pos = col_indx_pos,
    n_pos = n_pos,
    row_indx_na = row_indx_na,
    col_indx_na = col_indx_na,
    n_na = n_na,
    nu_fixed = nu_fixed,
    estimate_nu = as.integer(estimate_nu),
    use_normal = use_normal,
    est_cor = as.numeric(est_correlation),
    est_phi = as.numeric(estimate_trend_ar),
    est_theta = as.numeric(estimate_trend_ma),
    num_obs_covar = num_obs_covar,
    n_obs_covar = n_obs_covar,
    obs_covar_value = obs_covar_value,
    obs_covar_index = obs_covar_index,
    num_pro_covar = num_pro_covar,
    n_pro_covar = n_pro_covar,
    pro_covar_value = pro_covar_value,
    pro_covar_index = pro_covar_index,
    depvar = depvar,
    predictors = predictors
  )

  pars <- c("x", "Z", "sigma", "log_lik", "psi", "beta", "epsilon", "beta_recovered") # removed pred
  if (est_correlation) pars <- c(pars, "Omega") # add correlation matrix
  if (estimate_nu) pars <- c(pars, "nu")
  if (estimate_trend_ar) pars <- c(pars, "phi")
  if (estimate_trend_ma) pars <- c(pars, "theta")
  if(!is.null(obs_covar)) pars = c(pars, "b_obs")
  if(!is.null(pro_covar)) pars = c(pars, "b_pro")

  sampling_args <- list(
    object = stanmodels$dfa_pred,
    data = data_list,
    pars = pars,
    control = control,
    chains = chains,
    iter = iter,
    thin = thin,
    ...
  )

  if (sample) {
    mod <- do.call(sampling, sampling_args)
    # if (chains > 1) {
    #   out <- invert_chains(mod, trends = num_trends, print = FALSE)
    # } else {
    #   e <- rstan::extract(mod, permuted = FALSE)
    #   ep <- rstan::extract(mod, permuted = TRUE)
    #   out <- list(
    #     model = mod, samples_permuted = ep, samples = e,
    #     monitor = rstan::monitor(e)
    #   )
    # }
    # 
    # out[["data"]] <- Y # keep data included
    # out <- structure(out, class = "bayesdfa")
    out <- mod
  } else {
    out <- data_list
  }
  out
}

```

```{r}
a = scale(weekly_clean_data[, 7:16], center = T, scale = T)
depvar = scale(weekly_clean_data$total_sales, center = T, scale=T)[, 1]

mod = fit_dfa_pred(y = a,
  depvar = depvar,
  num_trends = 2,
  varIndx = NULL,
  zscore = T,
  iter = 5000,
  cores = 2,
  chains = 2,
  thin = 1,
  control = list(adapt_delta = 0.99, max_treedepth = 5),
  nu_fixed = 20,
  est_correlation = FALSE,
  estimate_nu = T,
  estimate_trend_ar = F,
  estimate_trend_ma = F,
  sample = TRUE,
  data_shape = "long",
  obs_covar = NULL,
  pro_covar = NULL,
  verbose = T
)
```


Turns out that you have to keep estimate_ma and ar terms = F


```{r}
post = rstan::extract(mod)
np = nuts_params(mod)
trends = post$x %>% apply(c(2,3), mean) %>% t

stan_trace(mod, "beta[1]")
post$x[, , ] %>% apply(c(2,3), mean) %>% t %>% .[, 1] %>% ts.plot()

```
```{r}
mcmc_parcoord(mod, np = np, pars = vars(starts_with("beta_recovered")))
print(mod, pars = "beta_recovered")

```


## Notes
- Okay so it seems that the inputs need to be scaled for this to work
- I thought I had run that model on dfa-pred, but I had not
- Dang, okay running it now

- Took foooorever. Going to run w/ 1000 iter. 
- Okay even w/ 100 iterations, this doesn't work. Time to follow the folk theorem

- The beta_k scale was wacky - pulled that down
- Still taking forever. Need to pare this back and go piece by piece. 
- Making beta a fixed vector

- Ok this is quite ridiculous. Taking out even beta and seeing what happens.
- Ok I hadn't commented out beta in the xformed params block so maybe that fucked things

- Okay now going to take beta out. 

- Ok the beta really makes it all run super slow. Damn it. 

- Not sure what the best way is forward from here. Let's try another beta run..
- Found some issues w/ what I was doing before. Hope this goes better. 
- Damn it--still gets suck. 

- Exploring a hunch that the model is unidentified with symmetry. Constraining beta > 0.
- Didn't seem to work. Fuck.

- Going to start with an init... 
- Still no joy

- GOD DAMN IT. 
- Okay going to do some step size jittering. 
- Okay nevermind betanalpha says not to

- Okay going to run it on some very very small data.
- Trying to follow this: https://discourse.mc-stan.org/t/large-model-runs-very-slowly/1325/4

- MAX TREEDEPTH WAS 20 WTFFF
- Okay reducing the treedepth does a lot
- The model does not appear to be well identified
- Need to look at the posterior draws and see which parameters are correlated
-> beta and Z, perhaps? 

- unclear, since there was so little movement in the parameters
- believe that i need to have a bigger step size in the adaptation phase

- i did -- but now I think that i need to constrain beta > 0

- new strategy -- i think i should perhaps let adapt_delta be small and see where there are divergences...

- new theory is that i need to qr decompose my model

data {
  int<lower=1> N;
  int<lower=1> M;
  matrix[M, N] X;
  // mine is transposed
  [N, M] X
  vector[N] y;
}

transformed data {
  // Compute, thin, and then scale QR decomposition
  matrix[N, M] Q = qr_Q(X')[, 1:M] * N;
  
  // so i don't need to transpose my X here...
  
  matrix[M, M] R = qr_R(X')[1:M, ] / N;
  matrix[M, M] R_inv = inverse(R);
}

Hmm -- that blew up gradient evaluation. That computation is way too expensive to be done inside transformed parameters

Maybe i do it just on the predictors? Let's see about that.

This did bring back down the gradient time, but why should this work in theory? The first factor is just likely to be a problem in any case

Okay early indications are positive here! Ran it with treedepth = 5 and adapt_delta = .9
Lots of divergences but I think that this is due to the stepsize being a bit larger
Can always bring the adapt_delta up

Okay bringing adapt delta essentially removed the issues but how TF would I do adstock xformation on a QR decomposed matrix... something for another time...

One additional thought of the night -- doing the QR decomposition on the matrix is pretty key and maybe worth the extra time in gradient evaluation

Also -- we don't need to esimate the effectiveness of the channels over the whole time period
Could take averages or something like that...

Jesus all of this just suxxx
Going to do the QR thing on our predictors again and blow up gradient computation time

Here is a good resource on multimodality:
https://discourse.mc-stan.org/t/help-with-qr-decomposition-with-latent-variable/14838/9

Okay wow the QR trick worked...

Going to back out what the betas actually are 

qr_thin way better -- interesting


## Next steps

- This *is scaled* 
- Also *this doesn't include adstocking!* 










