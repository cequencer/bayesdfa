---
title: "R Notebook"
output: html_notebook
---

- can't run this in bombas or any recast b/c recast project is pinned to rstan 2.18
- this model requires stanheaders 2.19.2
- stanheaders can't be above rstan

```{r setup}
library(tidyverse)
library(rstan)
library(bayesplot)
library(recast)
library(loo)
weekly_clean_data = read_csv("../../weekly_clean_data.csv")
dfa_pred = stan_model("../src/stan_files/dfa-pred.stan")
stanmodels <- list()
stanmodels$dfa_pred = dfa_pred
```

Because I didn't write my helper functions well below I need to store whatever the current dataset is in a global -- `current_data` -- which we'll overwrite. Not good practice but will suffice for testing. 

```{r}
data_list <- list()
data_list$full <- weekly_clean_data

# between four weeks of relatively calm period
data_list$t170 <- weekly_clean_data[1:170, ]
data_list$t174 <- weekly_clean_data[1:174, ]

# one week before the end of the data
data_list$t207 <- weekly_clean_data[1:207, ]

# resetting the global
current_data <- data_list$full
```


The first thing I need to do is to generate all the parameters for the factor part of the model:


```{r generate factor data}
.generate_factor_stan_data <- function(
                    y,
                    depvar,
                    num_trends = 1,
                    varIndx = NULL,
                    zscore = T,
                    nu_fixed = 101,
                    est_correlation = FALSE,
                    estimate_nu = FALSE,
                    estimate_trend_ar = FALSE,
                    estimate_trend_ma = FALSE,
                    sample = TRUE,
                    data_shape = "long",
                    obs_covar = NULL,
                    pro_covar = NULL) {
  predictors <- y
  
  data_shape <- match.arg(data_shape)
  if (ncol(y) > nrow(y) && data_shape == "long") {
    warning(
      "ncol(y) > nrow(y) and data_shape == 'long'; are you sure your",
      "input data is in long format?"
    )
  }
  if (ncol(y) < nrow(y) && data_shape == "wide") {
    warning(
      "ncol(y) < nrow(y) and data_shape == 'wide'; are you sure your",
      "input data is in wide format?"
    )
  }
  if (data_shape == "long") {
    y <- t(y)
  }

  if (nrow(y) < 3) {
    stop("fit_dfa() only works with 3 or more time series. We detected ",
      nrow(y), " time series.")
  }

  if(!is.null(obs_covar)) {
    if(ncol(obs_covar) != 4) {
      stop("observation covariates must be in a data frame with 4 columns")
    }
  }
  if(!is.null(pro_covar)) {
    if(ncol(pro_covar) != 4) {
      stop("process covariates must be in a data frame with 4 columns")
    }
  }

  # parameters for DFA
  N <- ncol(y) # number of time steps
  P <- nrow(y) # number of time series
  K <- num_trends # number of dfa trends
  nZ <- P * K - sum(seq_len(K)) # number of non-zero parameters that are unconstrained

  for (i in seq_len(P)) {
    if (zscore) {
      if (length(unique(na.omit(c(y[i, ])))) == 1L) {
        stop("Can't scale one or more of the time series because all values ",
          "are the same. Remove this/these time series or set `zscore = FALSE`.",
          call. = FALSE
        )
      }
      y[i, ] <- scale(y[i, ], center = TRUE, scale = TRUE)
    } else {
      y[i, ] <- scale(y[i, ], center = TRUE, scale = FALSE)
    }
  }
  Y <- y # included in returned object at end

  # mat_indx now references the unconstrained values of the Z matrix.
  mat_indx <- matrix(0, P, K)
  start <- 1
  for (k in seq_len(K)) {
    for (p in seq(k + 1, P)) {
      mat_indx[p, k] <- start
      start <- start + 1
    }
  }
  # row_indx and col_indx now references the unconstrained values of the Z matrix.
  row_indx <- matrix((rep(seq_len(P), K)), P, K)[mat_indx > 0]
  col_indx <- matrix(sort(rep(seq_len(K), P)), P, K)[mat_indx > 0]

  diag(mat_indx) <- 1
  row_indx_z <- matrix((rep(seq_len(P), K)), P, K)[mat_indx == 0]
  col_indx_z <- matrix(sort(rep(seq_len(K), P)), P, K)[mat_indx == 0]
  row_indx_z <- c(row_indx_z, 0, 0) # +2 zeros for making stan ok with data types
  col_indx_z <- c(col_indx_z, 0, 0) # +2 zeros for making stan ok with data types
  nZero <- length(row_indx_z)

  # set the model up to have shared variances
  if (is.null(varIndx)) {
    varIndx <- rep(1, P)
  }
  nVariances <- length(unique(varIndx))

  # indices of positive values - Stan can't handle NAs
  row_indx_pos <- matrix(rep(seq_len(P), N), P, N)[!is.na(y)]
  col_indx_pos <- matrix(sort(rep(seq_len(N), P)), P, N)[!is.na(y)]
  n_pos <- length(row_indx_pos)

  row_indx_na <- matrix(rep(seq_len(P), N), P, N)[is.na(y)]
  col_indx_na <- matrix(sort(rep(seq_len(N), P)), P, N)[is.na(y)]
  n_na <- length(row_indx_na)

  y <- y[!is.na(y)]

  # flag for whether to use a normal dist
  use_normal <- if (nu_fixed > 100) 1 else 0
  if (estimate_nu) use_normal <- 0 # competing flags

  # covariates
  if(!is.null(obs_covar)) {
    obs_covar_index = as.matrix(obs_covar[,c("time","timeseries","covariate")])
    num_obs_covar = nrow(obs_covar_index)
    n_obs_covar = length(unique(obs_covar_index[,"covariate"]))
    obs_covar_value = obs_covar[,"value"]
  } else {
    num_obs_covar = 0
    n_obs_covar = 0
    obs_covar_value = c(0)[0]
    obs_covar_index = matrix(0,1,3)[c(0)[0],]
  }
  if(!is.null(pro_covar)) {
    pro_covar_index = as.matrix(pro_covar[,c("time","trend","covariate")])
    num_pro_covar = nrow(pro_covar_index)
    n_pro_covar = length(unique(pro_covar_index[,"covariate"]))
    pro_covar_value = pro_covar[,"value"]
  } else {
    num_pro_covar = 0
    n_pro_covar = 0
    pro_covar_value = c(0)[0]
    pro_covar_index = matrix(0,1,3)[c(0)[0],]
  }

  data_list <- list(
    N = N,
    P = P,
    K = K,
    nZ = nZ,
    y = y,
    row_indx = row_indx,
    col_indx = col_indx,
    nZero = nZero,
    varIndx = varIndx,
    nVariances = nVariances,
    row_indx_z = row_indx_z,
    col_indx_z = col_indx_z,
    nZero = nZero,
    row_indx_z = row_indx_z,
    col_indx_z = col_indx_z,
    row_indx_pos = row_indx_pos,
    col_indx_pos = col_indx_pos,
    n_pos = n_pos,
    row_indx_na = row_indx_na,
    col_indx_na = col_indx_na,
    n_na = n_na,
    nu_fixed = nu_fixed,
    estimate_nu = as.integer(estimate_nu),
    use_normal = use_normal,
    est_cor = as.numeric(est_correlation),
    est_phi = as.numeric(estimate_trend_ar),
    est_theta = as.numeric(estimate_trend_ma),
    num_obs_covar = num_obs_covar,
    n_obs_covar = n_obs_covar,
    obs_covar_value = obs_covar_value,
    obs_covar_index = obs_covar_index,
    num_pro_covar = num_pro_covar,
    n_pro_covar = n_pro_covar,
    pro_covar_value = pro_covar_value,
    pro_covar_index = pro_covar_index,
    depvar = depvar,
    predictors = predictors
  )

  return(data_list)
}

generate_factor_stan_data <- function(depvar_name, chan_names, ...) {
  
  depvar = select(current_data, !!depvar_name)
  y      = as.data.frame(current_data[, chan_names])
  
  
  .generate_factor_stan_data(y, depvar, ...)
}

```

The second thing I need to do is to generate all the parameters for the custom filter part of the model: 

```{r generate channel parameters}

generate_obs_stan_data <- function(depvar_name, chan_names, num_trends, config_dir, upscale_trends = F) {

  y_actual = current_data[, depvar_name][[1]]
  
  if(file.exists(file.path(tempdir(), "test_sim.yaml"))) {
    file.remove(file.path(tempdir(), "test_sim.yaml"))
  }
  
  recast::create_default_chan_config(
    subset_name = "test",
    depvar_name = "sim",
    chan_names = chan_names, 
    config_dir = config_dir
  )

  orig_config = yaml::read_yaml(file.path(config_dir, "test_sim.yaml"))

  trend_template = orig_config[1]
  trend_template[[1]]$mu = 0.0001
  trend_template[[1]]$adstock = 0
  
  if(upscale_trends) {
    trend_template[[1]]$ub = Inf
    trend_template[[1]]$lb = -Inf
    trend_template[[1]]$q80 = sd(y_actual)/10
    trend_template[[1]]$q20 = -sd(y_actual)/10  
  }
  
  new_config = orig_config
  for(i in num_trends:1) {
    trend_component = trend_template
    names(trend_component)[1] = str_c("t", i)
    new_config = c(trend_component, new_config)
  }

  chan_params_factor = recast::longify_chan_config(new_config)
  
  other_data = list(
    sigma_level_ub = mean(y_actual) / 500,
    sigma_slope_ub = mean(y_actual) / 500,
    sigma_seasons_ub = mean(y_actual) / 100,
    sigma_adstock_ub = 100,
    measurement_endo_ub = mean(y_actual) / 50,
    measurement_exo_ub = mean(y_actual) / 50
  )
  
  all_data = c(chan_params_factor, other_data)
  
  return(all_data)
}

```

Now I need to put it all together:

```{r}

chan_names <- list()
factor_data <- list()

chan_names$small = colnames(current_data)[7:16]
depvar_name = "total_sales"
config_dir = tempdir()

factor_data$small = generate_factor_stan_data(depvar_name, chan_names$small, 
                                              num_trends = 3, estimate_nu = T, 
                                              zscore = T)

```

We're going to generate two stan datas, one with trends upscaled in our data/config and one in stan

First the way we have been doing it

```{r}

channel_data <- list()
stan_data <- list()

channel_data$scaled_in_config = generate_obs_stan_data(depvar_name, chan_names, 
                                                       num_trends = 3, config_dir, 
                                                       upscale_trends = T)

stan_data$scaled_in_config = c(factor_data$small, channel_data$scaled_in_config, 
                               n_holdout = 0, residualize = 0, upscale_trends = 0)

```

And second doing it in stan

```{r}

channel_data$scaled_in_stan = generate_obs_stan_data(depvar_name, chan_names, 
                                                       num_trends = 3, config_dir, 
                                                       upscale_trends = F)

stan_data$scaled_in_stan = c(factor_data$small, channel_data$scaled_in_stan, 
                               n_holdout = 0, residualize = 0, upscale_trends = 1)

```


And finally, to start sampling -- first the existing way:

```{r}

fit <- list()
post <- list()

# fit$scaled_in_config = vb(
#   stanmodels$dfa_pred,
#   stan_data$scaled_in_config,
#   # iter = 100,
#   # chains = 2,
#   # cores = 2,
#   # control = list(adapt_delta = .99, max_treedepth = 10),
#   pars = c("channel_spend_impact", "log_lik", "x", "y_pred", "Z", "rates", "Px_diag")
# )
# 
# post$scaled_in_config = rstan::extract(fit$scaled_in_config)

```

And with residualization:

```{r}
# fit$scaled_in_stan = vb(
#   stanmodels$dfa_pred,
#   stan_data$scaled_in_stan,
#   # iter = 100,
#   # chains = 2,
#   # cores = 2,
#   # control = list(adapt_delta = .99, max_treedepth = 10),
#   pars = c("channel_spend_impact", "log_lik", "x", "y_pred", "Z", "rates", "Px_diag")
# )
# 
# post$scaled_in_stan = rstan::extract(fit$scaled_in_stan)
```


```{r}
log_lik <- list()
r_eff   <- list()
leeloo  <- list()

# log_lik$scaled_in_config <- extract_log_lik(fit$scaled_in_config, merge_chains = FALSE)
# r_eff$scaled_in_config <- relative_eff(exp(log_lik$scaled_in_config ))
# leeloo$scaled_in_config <- loo(log_lik$scaled_in_config, r_eff = r_eff$scaled_in_config, cores = 2)
# 
# log_lik$scaled_in_stan <- extract_log_lik(fit$scaled_in_stan, merge_chains = FALSE)
# r_eff$scaled_in_stan <- relative_eff(exp(log_lik$scaled_in_stan))
# leeloo$scaled_in_stan <- loo(log_lik$scaled_in_stan, r_eff = r_eff$scaled_in_stan, cores = 2)

```

```{r}

plot_comp_between_channels = 
  function(idx, post_list, n_factors) {
    map2(post_list, n_factors, ~ .x$channel_spend_impact %>% apply(c(2,3), mean) %>% .[, .y + idx]) %>% 
    reduce(cbind) %>% 
    ts.plot(col = c("darkred", "darkblue"))
    
    legend("topright", names(post_list), col = c("darkred", "darkblue"), lty = 1)
  }
```


```{r}
# ts.plot(current_data$total_sales)
# abline(v = loo::pareto_k_ids(leeloo$scaled_in_config, threshold = 1), col = "darkred")
```


Now going to try with the full complement of channels -- now testing three and five latent factors 

```{r}

chan_names$large = colnames(current_data)[3:24]

factor_data$large$three = generate_factor_stan_data(depvar_name, 
                                              chan_names$large, num_trends = 3, estimate_nu = T, 
                                              zscore = T)

factor_data$large$five = generate_factor_stan_data(depvar_name, 
                                              chan_names$large, num_trends = 5, estimate_nu = T, 
                                              zscore = T)


channel_data$large$three = generate_obs_stan_data(depvar_name, chan_names$large, 
                                                       num_trends = 3, config_dir, 
                                                       upscale_trends = T)

stan_data$large$three = c(factor_data$large$three, channel_data$large$three, 
                               n_holdout = 0, residualize = 0, upscale_trends = 0)

channel_data$large$five = generate_obs_stan_data(depvar_name, chan_names$large, 
                                                       num_trends = 5, config_dir, 
                                                       upscale_trends = T)

stan_data$large$five = c(factor_data$large$five, channel_data$large$five, 
                               n_holdout = 0, residualize = 0, upscale_trends = 0)

```

```{r}
fit$large$five = vb(
  stanmodels$dfa_pred,
  stan_data$large$five,
  # iter = 100,
  # chains = 2,
  # cores = 2,
  # control = list(adapt_delta = .99, max_treedepth = 10),
  pars = c("channel_spend_impact", "log_lik", "x", "y_pred", "Z", "rates", "Px_diag")
)

post$large$five = rstan::extract(fit$large$five)
```

```{r}
fit$large$three = vb(
  stanmodels$dfa_pred,
  stan_data$large$three,
  # iter = 100,
  # chains = 2,
  # cores = 2,
  # control = list(adapt_delta = .99, max_treedepth = 10),
  pars = c("channel_spend_impact", "log_lik", "x", "y_pred", "Z", "rates", "Px_diag")
)

post$large$three = rstan::extract(fit$large$three)
```

```{r}
log_lik$large$five <- extract_log_lik(fit$large$five, merge_chains = FALSE)
r_eff$large$five   <- relative_eff(exp(log_lik$large$five))
leeloo$large$five  <- loo(log_lik$large$five, r_eff = r_eff$large$five, cores = 2)

log_lik$large$three <- extract_log_lik(fit$large$three, merge_chains = FALSE)
r_eff$large$three   <- relative_eff(exp(log_lik$large$three))
leeloo$large$three  <- loo(log_lik$large$three, r_eff = r_eff$large$three, cores = 2)

loo_compare(leeloo$large)
loo::loo_model_weights(leeloo$large)
```

```{r}
ts.plot(current_data$total_sales)
abline(v = loo::pareto_k_ids(leeloo$large$three, threshold = .7), col = "darkred")
abline(v = loo::pareto_k_ids(leeloo$large$five, threshold = .7), col = "darkblue")
```

```{r}
plot_comp_between_channels(10, post$large, c(5, 3))
```


Let's run the model up to various stopping points

```{r}
current_data <- data_list$t207
chan_names$stopping = colnames(current_data)[3:24]

factor_data$stopping$t207 = generate_factor_stan_data(depvar_name, 
                                              chan_names$stopping, num_trends = 3, 
                                              estimate_nu = T, 
                                              zscore = T)

channel_data$stopping$t207 = generate_obs_stan_data(depvar_name, chan_names$stopping, 
                                                       num_trends = 3, config_dir, 
                                                       upscale_trends = T)

stan_data$stopping$t207 = c(factor_data$stopping$t207, channel_data$stopping$t207, 
                               n_holdout = 0, residualize = 0, upscale_trends = 0)
```

```{r}
fit$stopping$t207 = vb(
  stanmodels$dfa_pred,
  stan_data$stopping$t207,
  # iter = 100,
  # chains = 2,
  # cores = 2,
  # control = list(adapt_delta = .99, max_treedepth = 10),
  pars = c("channel_spend_impact", "log_lik", "x", "y_pred", "Z", "rates", "Px_diag")
)

post$stopping$t207 = rstan::extract(fit$stopping$t207)
```

```{r}

cbind(
  post$stopping$t207$channel_spend_impact %>% apply(c(2,3), mean) %>% .[, 4],
  post$large$three$channel_spend_impact %>% apply(c(2,3), mean) %>% .[-208, 4]
) %>% 
  ts.plot()



```

How does the model do on DW? 

```{r}
cbind.data.frame(
  actual   = weekly_clean_data$total_sales,
  predicted = post$large$three$y_pred %>% apply(c(2,3), mean)
) %>% 
  with(dwtest(actual~predicted))
```

What does Loo say about looking at only the last 100 days of data



```{r}
leeloo$large$three_sub = loo::loo(log_lik$large$three[, , 100:208], r_eff = r_eff$large$three[100:208], cores = 2)
```
Do our poorly estimated time series correspond with big jumps in the trends?

```{r}
ts.plot(weekly_clean_data$total_sales)
abline(v = (loo::pareto_k_ids(leeloo$large$three_sub) + 99), col = "darkred")
```

Let's rotate the trends

```{r}
Z = post$large$three$Z %>% apply(c(2,3), mean)
unrottrends = post$large$three$x %>% apply(c(2,3), mean)
Hinv = varimax(Z)$rotmat
trends = t(solve(Hinv) %*% unrottrends)
dtrends = trends %>% apply(2, diff)
emp_sd = dtrends %>% apply(2, sd)
which((1-pnorm(abs(dtrends[, 1]), 0, sd = emp_sd[1])) < 0.01)
which((1-pnorm(abs(dtrends[, 2]), 0, sd = emp_sd[2])) < 0.01)
which((1-pnorm(abs(dtrends[, 3]), 0, sd = emp_sd[3])) < 0.01)
(loo::pareto_k_ids(leeloo$large$three_sub) + 100)

ts.plot(weekly_clean_data$total_sales)
abline(v = (loo::pareto_k_ids(leeloo$large$three_sub) + 99), col = "darkred")
abline(v = which((1-pnorm(abs(dtrends[, 2]), 0, sd = emp_sd[2])) < 0.01), col = "darkblue")
legend("topleft", c("sales", "poorly estim points", "swans"), 
       col = c("black", "darkred", "darkblue"), lty = 1)
```

Tests
1. Run new model with the away data
2. Run model with bombas data + noise
3. Run model with pure noise
4. Run bombas + smoothing


## testing with noise

```{r}
data_list$noise <- data_list$full

data_list$noise$noise <- cumsum(rnorm(nrow(data_list$full), 0, sd = 10))

current_data <- data_list$noise

chan_names$noise = colnames(current_data)[c(3:24, 38)]

factor_data$noise = generate_factor_stan_data(depvar_name, 
                                              chan_names$noise, num_trends = 3, 
                                              estimate_nu = T, 
                                              zscore = T)

channel_data$noise = generate_obs_stan_data(depvar_name, chan_names$noise, 
                                                       num_trends = 3, config_dir, 
                                                       upscale_trends = T)

stan_data$noise = c(factor_data$noise, channel_data$noise, 
                               n_holdout = 0, residualize = 0, upscale_trends = 0)

stan_data$noise$x1_exo[26] = 0
stan_data$noise$state_bound_type[26] = 3
stan_data$noise$Px1_vector[26]
```

```{r}
fit$noise = vb(
  stanmodels$dfa_pred,
  stan_data$noise,
  # iter = 100,
  # chains = 2,
  # cores = 2,
  # control = list(adapt_delta = .99, max_treedepth = 10),
  pars = c("channel_spend_impact", "log_lik", "x", "y_pred", "Z", "rates", "Px_diag")
)

post$noise = rstan::extract(fit$noise)
```

Is it actually noise?

```{r}
m = post$noise$channel_spend_impact %>% apply(c(2,3), mean) %>% .[, 26] 
s = post$noise$Px_diag %>% apply(c(2,3), mean) %>% .[, 26] 

ts.plot(cbind(m+2*s, m, m-2*s), col = c("darkred", "black", "darkred"), lty = c(2,1,2))

```

Okay, now going to test Away data

```{r}

data_list$away <- read_csv("../../hermes20200512.csv")

current_data <- data_list$away

colnames(current_data)

chan_names$away = colnames(current_data)[c(8, 10:35)]

factor_data$away = generate_factor_stan_data(depvar_name, 
                                              chan_names$away, num_trends = 3, 
                                              estimate_nu = T, 
                                              zscore = T)

channel_data$away = generate_obs_stan_data(depvar_name, chan_names$away, 
                                                       num_trends = 3, config_dir, 
                                                       upscale_trends = T)

stan_data$away = c(factor_data$away, channel_data$away, 
                               n_holdout = 0, residualize = 0, upscale_trends = 0)

stan_data$away$x1_exo[29] = 0
stan_data$away$state_bound_type[29] = 3
stan_data$away$Px1_vector[29]


```

```{r}
fit$away = vb(
  stanmodels$dfa_pred,
  stan_data$away,
  # iter = 100,
  # chains = 2,
  # cores = 2,
  # control = list(adapt_delta = .99, max_treedepth = 10),
  pars = c("channel_spend_impact", "log_lik", "x", "y_pred", "Z", "rates", "Px_diag")
)

post$away = rstan::extract(fit$away)
```

```{r}
post$away$channel_spend_impact %>% apply(c(2,3), mean) %>% .[, 29] %>% ts.plot()
```


```{r}
tmp <- 1:length(colnames(data_list$away[, chan_names$away])) + 3
names(tmp) <- colnames(data_list$away[, chan_names$away])

(tmp2 <- tmp[data_list$away[, chan_names$away] %>% 
  apply(2, mean) %>% 
  sort(d = T) %>% 
  names])
  

idx = 11
m = post$away$channel_spend_impact %>% apply(c(2,3), mean) %>% .[, idx] 
s = post$away$Px_diag %>% apply(c(2,3), mean) %>% .[, idx] %>% sqrt

ts.plot(cbind(m+2*s, m, m-2*s), col = c("darkred", "black", "darkred"), lty = c(2,1,2), main = names(tmp2)[which(tmp2 == idx)])
```



```{r}
y_pred = post$away$y_pred %>% apply(c(2,3), mean)

cbind(y_pred, factor_data$away$depvar) %>% 
  ts.plot(col = c("darkred", "darkblue"))

cbind.data.frame(pred = y_pred, actual = factor_data$away$depvar) %>% 
  with(., dwtest(total_sales~pred))

cbind.data.frame(pred = y_pred, actual = factor_data$away$depvar) %>% 
  mutate(res = total_sales-pred) %>% 
  pull(res) %>% 
  sd %>% 
  cat("RMSE is", scales::dollar(.), "\n")

cbind.data.frame(pred = y_pred, actual = factor_data$away$depvar) %>% 
  mutate(res = total_sales-pred) %>% 
  pull(res) %>% 
  ts.plot()

cbind.data.frame(pred = y_pred, actual = factor_data$away$depvar) %>% 
  mutate(res = total_sales-pred) %>% 
  mutate(ape = abs(res)/total_sales) %>% 
  pull(ape) %>% 
  mean %>% 
  cat("MAPE is", scales::percent(.), "\n")
```



```{r}
log_lik$away <- extract_log_lik(fit$away, merge_chains = FALSE)
r_eff$away <- relative_eff(exp(log_lik$away ))
leeloo$away <- loo(log_lik$away, r_eff = r_eff$away, cores = 2)
leeloo$away

pareto_k_ids(leeloo$away, threshold = 1)

ts.plot(factor_data$away$depvar)
abline(v = pareto_k_ids(leeloo$away, threshold = .7), col = "darkred")
```



```{r}
fit_dfa <- function(y = y,
                    num_trends = 1,
                    varIndx = NULL,
                    zscore = TRUE,
                    iter = 2000,
                    chains = 4,
                    thin = 1,
                    control = list(adapt_delta = 0.99, max_treedepth = 20),
                    nu_fixed = 101,
                    est_correlation = FALSE,
                    estimate_nu = FALSE,
                    estimate_trend_ar = FALSE,
                    estimate_trend_ma = FALSE,
                    sample = TRUE,
                    data_shape = c("wide", "long"),
                    obs_covar = NULL,
                    pro_covar = NULL,
                    ...) {
  data_shape <- match.arg(data_shape)
  if (ncol(y) > nrow(y) && data_shape == "long") {
    warning(
      "ncol(y) > nrow(y) and data_shape == 'long'; are you sure your",
      "input data is in long format?"
    )
  }
  if (ncol(y) < nrow(y) && data_shape == "wide") {
    warning(
      "ncol(y) < nrow(y) and data_shape == 'wide'; are you sure your",
      "input data is in wide format?"
    )
  }
  if (data_shape == "long") {
    y <- t(y)
  }

  if (nrow(y) < 3) {
    stop("fit_dfa() only works with 3 or more time series. We detected ",
      nrow(y), " time series.")
  }

  if(!is.null(obs_covar)) {
    if(ncol(obs_covar) != 4) {
      stop("observation covariates must be in a data frame with 4 columns")
    }
  }
  if(!is.null(pro_covar)) {
    if(ncol(pro_covar) != 4) {
      stop("process covariates must be in a data frame with 4 columns")
    }
  }

  # parameters for DFA
  N <- ncol(y) # number of time steps
  P <- nrow(y) # number of time series
  K <- num_trends # number of dfa trends
  nZ <- P * K - sum(seq_len(K)) # number of non-zero parameters that are unconstrained

  for (i in seq_len(P)) {
    if (zscore) {
      if (length(unique(na.omit(c(y[i, ])))) == 1L) {
        stop("Can't scale one or more of the time series because all values ",
          "are the same. Remove this/these time series or set `zscore = FALSE`.",
          call. = FALSE
        )
      }
      y[i, ] <- scale(y[i, ], center = TRUE, scale = TRUE)
    } else {
      y[i, ] <- scale(y[i, ], center = TRUE, scale = FALSE)
    }
  }
  Y <- y # included in returned object at end

  # mat_indx now references the unconstrained values of the Z matrix.
  mat_indx <- matrix(0, P, K)
  start <- 1
  for (k in seq_len(K)) {
    for (p in seq(k + 1, P)) {
      mat_indx[p, k] <- start
      start <- start + 1
    }
  }
  # row_indx and col_indx now references the unconstrained values of the Z matrix.
  row_indx <- matrix((rep(seq_len(P), K)), P, K)[mat_indx > 0]
  col_indx <- matrix(sort(rep(seq_len(K), P)), P, K)[mat_indx > 0]

  diag(mat_indx) <- 1
  row_indx_z <- matrix((rep(seq_len(P), K)), P, K)[mat_indx == 0]
  col_indx_z <- matrix(sort(rep(seq_len(K), P)), P, K)[mat_indx == 0]
  row_indx_z <- c(row_indx_z, 0, 0) # +2 zeros for making stan ok with data types
  col_indx_z <- c(col_indx_z, 0, 0) # +2 zeros for making stan ok with data types
  nZero <- length(row_indx_z)

  # set the model up to have shared variances
  if (is.null(varIndx)) {
    varIndx <- rep(1, P)
  }
  nVariances <- length(unique(varIndx))

  # indices of positive values - Stan can't handle NAs
  row_indx_pos <- matrix(rep(seq_len(P), N), P, N)[!is.na(y)]
  col_indx_pos <- matrix(sort(rep(seq_len(N), P)), P, N)[!is.na(y)]
  n_pos <- length(row_indx_pos)

  row_indx_na <- matrix(rep(seq_len(P), N), P, N)[is.na(y)]
  col_indx_na <- matrix(sort(rep(seq_len(N), P)), P, N)[is.na(y)]
  n_na <- length(row_indx_na)

  y <- y[!is.na(y)]

  # flag for whether to use a normal dist
  use_normal <- if (nu_fixed > 100) 1 else 0
  if (estimate_nu) use_normal <- 0 # competing flags

  # covariates
  if(!is.null(obs_covar)) {
    obs_covar_index = as.matrix(obs_covar[,c("time","timeseries","covariate")])
    num_obs_covar = nrow(obs_covar_index)
    n_obs_covar = length(unique(obs_covar_index[,"covariate"]))
    obs_covar_value = obs_covar[,"value"]
  } else {
    num_obs_covar = 0
    n_obs_covar = 0
    obs_covar_value = c(0)[0]
    obs_covar_index = matrix(0,1,3)[c(0)[0],]
  }
  if(!is.null(pro_covar)) {
    pro_covar_index = as.matrix(pro_covar[,c("time","trend","covariate")])
    num_pro_covar = nrow(pro_covar_index)
    n_pro_covar = length(unique(pro_covar_index[,"covariate"]))
    pro_covar_value = pro_covar[,"value"]
  } else {
    num_pro_covar = 0
    n_pro_covar = 0
    pro_covar_value = c(0)[0]
    pro_covar_index = matrix(0,1,3)[c(0)[0],]
  }

  data_list <- list(
    N = N,
    P = P,
    K = K,
    nZ = nZ,
    y = y,
    row_indx = row_indx,
    col_indx = col_indx,
    nZero = nZero,
    varIndx = varIndx,
    nVariances = nVariances,
    row_indx_z = row_indx_z,
    col_indx_z = col_indx_z,
    nZero = nZero,
    row_indx_z = row_indx_z,
    col_indx_z = col_indx_z,
    row_indx_pos = row_indx_pos,
    col_indx_pos = col_indx_pos,
    n_pos = n_pos,
    row_indx_na = row_indx_na,
    col_indx_na = col_indx_na,
    n_na = n_na,
    nu_fixed = nu_fixed,
    estimate_nu = as.integer(estimate_nu),
    use_normal = use_normal,
    est_cor = as.numeric(est_correlation),
    est_phi = as.numeric(estimate_trend_ar),
    est_theta = as.numeric(estimate_trend_ma),
    num_obs_covar = num_obs_covar,
    n_obs_covar = n_obs_covar,
    obs_covar_value = obs_covar_value,
    obs_covar_index = obs_covar_index,
    num_pro_covar = num_pro_covar,
    n_pro_covar = n_pro_covar,
    pro_covar_value = pro_covar_value,
    pro_covar_index = pro_covar_index
  )

  pars <- c("x", "Z", "sigma", "log_lik", "psi") # removed pred
  if (est_correlation) pars <- c(pars, "Omega") # add correlation matrix
  if (estimate_nu) pars <- c(pars, "nu")
  if (estimate_trend_ar) pars <- c(pars, "phi")
  if (estimate_trend_ma) pars <- c(pars, "theta")
  if(!is.null(obs_covar)) pars = c(pars, "b_obs")
  if(!is.null(pro_covar)) pars = c(pars, "b_pro")

  sampling_args <- list(
    object = stanmodels$dfa,
    data = data_list,
    pars = pars,
    control = control,
    chains = chains,
    iter = iter,
    thin = thin,
    ...
  )

  if (sample) {
    mod <- do.call(sampling, sampling_args)
    if (chains > 1) {
      out <- invert_chains(mod, trends = num_trends, print = FALSE)
    } else {
      e <- rstan::extract(mod, permuted = FALSE)
      ep <- rstan::extract(mod, permuted = TRUE)
      out <- list(
        model = mod, samples_permuted = ep, samples = e,
        monitor = rstan::monitor(e)
      )
    }

    out[["data"]] <- Y # keep data included
    out <- structure(out, class = "bayesdfa")
  } else {
    out <- data_list
  }
  out
}



```

```{r}
sim_dfa <- function(num_trends = 1,
                    num_years = 20,
                    num_ts = 4,
                    loadings_matrix = matrix(
                      nrow = num_ts, ncol = num_trends,
                      rnorm(num_ts * num_trends, 0, 1)
                    ),
                    sigma = rlnorm(1, meanlog = log(0.2), 0.1),
                    varIndx = rep(1, num_ts),
                    extreme_value = NULL,
                    extreme_loc = NULL,
                    nu_fixed = 100,
                    user_supplied_deviations = NULL) {
  y_ignore <- matrix(rnorm(num_ts * num_years), nrow = num_ts, ncol = num_years)

  d <- fit_dfa(y_ignore,
    num_trends = num_trends, sample = FALSE, zscore = FALSE,
    varIndx = varIndx, nu_fixed = nu_fixed
  )

  Z <- loadings_matrix
  y <- vector(mode = "numeric", length = d$N)

  for (k in seq_len(d$K)) {
    Z[k, k] <- abs(Z[k, k]) # add constraint for Z diagonal
  }
  # fill in 0s
  for (k in seq_len(d$K)) {
    for (p in seq_len(d$P)) {
      if (p < k) Z[p, k] <- 0
    }
  }

  x <- matrix(nrow = d$K, ncol = d$N) # random walk-trends


  # initial state for each trend
  for (k in seq_len(d$K)) {

    if (!is.null(user_supplied_deviations)) {
      devs <- user_supplied_deviations[,k]
    } else {
      devs <- rt(d$N, df = d$nu_fixed)
    }

    x[k, 1] <- rnorm(1, 0, 1)
    if (is.null(extreme_value)) {
      for (t in seq(2, d$N)) {
        x[k, t] <- x[k, t - 1] + devs[t] # random walk
      }
    } else {
      if (is.null(extreme_loc)) extreme_loc <- round(num_years / 2)
      for (t in 2:(extreme_loc - 1)) {
        x[k, t] <- x[k, t - 1] + devs[t] # random walk
      }
      # only include extreme in first trend
      if (k == 1) {
        x[1, extreme_loc] <- x[1, extreme_loc - 1] + extreme_value
      } else {
        x[k, extreme_loc] <- x[k, extreme_loc - 1] + devs[t]
      }
      for (t in seq(extreme_loc + 1, d$N)) {
        x[k, t] <- x[k, t - 1] + devs[t] # random walk
      }
    }
  }

  pred <- Z %*% x
  for (i in seq_len(d$n_pos)) {
    y[i] <- rnorm(
      1, pred[d$row_indx_pos[i], d$col_indx_pos[i]],
      sigma[d$varIndx[d$row_indx_pos[i]]]
    )
  }
  y_sim <- matrix(y, nrow = d$P)
  list(y_sim = y_sim, pred = pred, x = x, Z = Z, sigma = sigma)
}
```


```{r}
sim_dat_raw = sim_dfa(num_trends = 3, num_years = 200, num_ts = 11)

sim_dat = t(sim_dat_raw$y_sim) %>% 
  as_tibble %>% 
  setNames(c("total_sales", str_c("s", 1:10)))

sim_dat %>% head


data_list$sim <- sim_dat

current_data <- data_list$sim

colnames(current_data)

chan_names$sim = str_c("s", 1:10)

factor_data$sim = generate_factor_stan_data(depvar_name, 
                                              chan_names$sim, num_trends = 3, 
                                              estimate_nu = T, 
                                              zscore = T)

channel_data$sim = generate_obs_stan_data(depvar_name, chan_names$sim, 
                                                       num_trends = 3, config_dir, 
                                                       upscale_trends = T)


channel_data$sim$x1_exo[1:13] = 0
channel_data$sim$state_bound_type[1:13] = 0

stan_data$sim = c(factor_data$sim, channel_data$sim, 
                               n_holdout = 0, residualize = 0, upscale_trends = 0)

stan_data$sim$state_lb[1:3] = 0
stan_data$sim$state_ub[1:3] = 0
stan_data$sim$depvar %>% ts.plot()
stan_data$sim %>% 
  map(~ any(abs(.x - -0.0174605)<0.001)) %>% 
  purrr::keep(~.x)

stan_data$sim$measurement_exo_ub = abs(stan_data$sim$measurement_exo_ub)
stan_data$sim$measurement_endo_ub = abs(stan_data$sim$measurement_endo_ub)
```

```{r}
fit$sim = vb(
  stanmodels$dfa_pred,
  stan_data$sim,
  # iter = 100,
  # chains = 2,
  # cores = 2,
  # control = list(adapt_delta = .99, max_treedepth = 10),
  pars = c("channel_spend_impact", "log_lik", "x", "y_pred", "Z", "rates", "Px_diag")
)

post$sim = rstan::extract(fit$sim)
```

```{r}
m = post$sim$channel_spend_impact %>% apply(c(2,3), mean)
s = post$sim$Px_diag %>% apply(c(2,3), mean) %>% sqrt
```

```{r}
idx = 5
ts.plot(cbind(m[,idx]+2*s[,idx], m[,idx], m[idx]-2*s[,idx]),
        col = c("darkred", "black", "darkred"), lty = c(2,1,2))
```

